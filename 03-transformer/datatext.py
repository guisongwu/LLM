texts = [
    # Greetings (repeat patterns)
    "hello world.",
    "hello there.",
    "hi there.",
    "good morning.",
    "good evening.",
    "hello world again.",
    "hi world.",
    "greetings friend.",
    "hello friend.",
    "good afternoon.",
    "hello everyone.",
    "hi everyone.",
    "greetings to all.",
    "welcome friend.",
    "hello again.",
    "hi again.",
    "good night.",
    "hello buddy.",
    "hi buddy.",
    "greetings buddy.",

    # Machine learning basics (repeat with variations)
    "machine learning is fun.",
    "machine learning is amazing.",
    "deep learning is fun.",
    "deep learning is powerful.",
    "neural networks are fun to train.",
    "neural networks learn from data.",
    "gradient descent optimizes models.",
    "optimization improves models.",
    "loss functions measure errors.",
    "accuracy measures performance.",
    "models learn from data.",
    "training improves machine learning models.",
    "deep learning models are powerful.",
    "neural networks can predict outputs.",
    "machine learning predicts labels.",
    "training requires lots of data.",
    "training adjusts model parameters.",
    "models generalize from examples.",
    "learning patterns is exciting.",
    "neural networks learn patterns.",
    # Repeat above patterns multiple times with small variations to reach ~60-70 sentences
    "machine learning is fun and exciting.",
    "deep learning helps neural networks learn.",
    "gradient descent finds minima of loss functions.",
    "accuracy evaluates model performance.",
    "models improve with more examples.",
    "training neural networks requires patience.",
    "optimization algorithms adjust parameters.",
    "loss functions guide the training.",
    "neural networks can learn features.",
    "machine learning models make predictions.",
    "deep learning models require compute.",
    "neural networks approximate functions.",
    "training improves predictions.",
    "evaluation checks model accuracy.",
    "validation prevents overfitting.",

    # Transformers and attention (repeat patterns)
    "transformers are powerful.",
    "transformers are amazing.",
    "attention is all you need.",
    "self attention focuses on tokens.",
    "multihead attention improves context.",
    "positional encoding adds order.",
    "language models generate text.",
    "language models predict next words.",
    "tokenization splits text into tokens.",
    "embedding layers map tokens to vectors.",
    "transformers process sequences.",
    "attention captures dependencies.",
    "language models learn patterns.",
    "self attention improves predictions.",
    "transformers replace RNNs.",
    "transformers are used in NLP.",
    "transformers are fun to study.",
    # Repeat patterns with variations
    "multihead attention allows parallel focus.",
    "self attention captures relationships.",
    "positional encoding encodes positions.",
    "transformers model sequences efficiently.",
    "language models predict tokens.",
    "tokenization prepares data for models.",
    "embedding layers create dense representations.",
    "transformers process long sequences.",
    "attention mechanisms improve context awareness.",
    "transformers handle long dependencies.",
    "language models can generate coherent text.",
    "self attention helps transformers learn.",
    "transformers outperform RNNs in NLP.",
    "positional encoding maintains order.",
    "multihead attention combines multiple perspectives.",
    "transformers are popular in AI research.",
    "language models learn from large corpora.",
    "embedding layers map words to vectors.",
    "transformers enable powerful NLP models.",
    "self attention allows context-sensitive predictions.",
    "attention mechanisms focus on important tokens.",

    # Training & evaluation (repeat patterns)
    "training requires data.",
    "training adjusts parameters.",
    "evaluation measures performance.",
    "overfitting occurs on small data.",
    "regularization prevents overfitting.",
    "dropout prevents co-adaptation.",
    "batch normalization stabilizes training.",
    "large datasets improve generalization.",
    "learning rate affects convergence.",
    "optimizers help models learn.",
    "models improve with more data.",
    "training requires multiple epochs.",
    "evaluation checks model accuracy.",
    "validation helps prevent overfitting.",
    # Repeat patterns with variations
    "training neural networks requires data.",
    "overfitting happens on tiny datasets.",
    "regularization techniques prevent overfitting.",
    "dropout improves generalization.",
    "batch normalization helps optimization.",
    "large datasets improve model performance.",
    "learning rate impacts training.",
    "optimizers guide model parameters.",
    "models learn faster with more data.",
    "evaluation metrics measure correctness.",
    "validation helps tune hyperparameters.",
    "training epochs improve performance.",
    "loss functions guide optimization.",
    "accuracy measures model predictions.",
    "metrics help evaluate models.",

    # Reinforcement & other learning (repeat patterns)
    "reinforcement learning trains agents.",
    "supervised learning uses labels.",
    "unsupervised learning finds patterns.",
    "models learn from examples.",
    "predicting the next token is fun.",
    "agents learn from rewards.",
    "reinforcement learning improves policies.",
    "supervised learning learns from targets.",
    "unsupervised learning clusters data.",
    "examples teach models patterns.",
    # Repeat variations
    "agents improve behavior through rewards.",
    "supervised learning uses annotated data.",
    "unsupervised learning detects patterns.",
    "predicting sequences is fun.",
    "models generalize from examples.",
    "training improves agent policies.",
    "reinforcement learning uses feedback.",
    "examples guide model learning.",
    "supervised models learn mappings.",
    "unsupervised models cluster inputs.",
    "agents act to maximize rewards.",
    "reinforcement improves agent performance.",
    "training teaches models patterns.",
    "examples improve model predictions.",

    # Misc patterns (repeat)
    "AI is transforming the world.",
    "AI changes the way we live.",
    "technology evolves rapidly.",
    "computers process information.",
    "data is the new oil.",
    "information drives decisions.",
    "algorithms help solve problems.",
    "models can generate text.",
    "models can complete sentences.",
    "learning patterns is exciting.",
    # Repeat variations
    "AI impacts daily life.",
    "technology advances quickly.",
    "data drives AI research.",
    "algorithms process information.",
    "models learn patterns efficiently.",
    "AI enables new applications.",
    "models can predict outputs.",
    "computers handle complex tasks.",
    "learning from data is essential.",
    "models generalize from examples.",
    "technology transforms industries.",
    "AI changes workflows.",
    "models assist humans in tasks.",
    "data fuels machine learning.",
    "AI enables automation.",
    "learning from examples improves models.",
]





texts += [
    # =================
    # Greetings (60 sentences)
    "hello world.",
    "hello there.",
    "hi there.",
    "good morning.",
    "good evening.",
    "hello world again.",
    "hi world.",
    "greetings friend.",
    "hello friend.",
    "good afternoon.",
    "hello everyone.",
    "hi everyone.",
    "greetings to all.",
    "welcome friend.",
    "hello again.",
    "hi again.",
    "good night.",
    "hello buddy.",
    "hi buddy.",
    "greetings buddy.",
    "hello world, again.",
    "hi world, again.",
    "hello friend, again.",
    "greetings everyone.",
    "good morning friend.",
    "good evening friend.",
    "hello everyone again.",
    "hi everyone again.",
    "greetings to all again.",
    "welcome back friend.",
    "hello again friend.",
    "hi again friend.",
    "good night friend.",
    "hello buddy again.",
    "hi buddy again.",
    "greetings buddy again.",
    "hello world once more.",
    "hi world once more.",
    "hello friend once more.",
    "greetings everyone once more.",
    "good morning again.",
    "good evening again.",
    "hello everyone once again.",
    "hi everyone once again.",
    "greetings to all once again.",
    "welcome back everyone.",
    "hello again everyone.",
    "hi again everyone.",
    "good night again.",
    "hello buddy once more.",
    "hi buddy once more.",
    "greetings buddy once more.",
    "hello world my friend.",
    "hi world my friend.",
    "hello friend my friend.",
    "greetings everyone my friend.",
    "good morning my friend.",
    "good evening my friend.",
    "hello everyone my friend.",
    "hi everyone my friend.",
    "greetings to all my friend.",

    # =================
    # Machine Learning basics (100 sentences)
    "machine learning is fun.",
    "machine learning is amazing.",
    "deep learning is fun.",
    "deep learning is powerful.",
    "neural networks are fun to train.",
    "neural networks learn from data.",
    "gradient descent optimizes models.",
    "optimization improves models.",
    "loss functions measure errors.",
    "accuracy measures performance.",
    "models learn from data.",
    "training improves machine learning models.",
    "deep learning models are powerful.",
    "neural networks can predict outputs.",
    "machine learning predicts labels.",
    "training requires lots of data.",
    "training adjusts model parameters.",
    "models generalize from examples.",
    "learning patterns is exciting.",
    "neural networks learn patterns.",
    # Variations
    "machine learning is fun and exciting.",
    "deep learning helps neural networks learn.",
    "gradient descent finds minima of loss functions.",
    "accuracy evaluates model performance.",
    "models improve with more examples.",
    "training neural networks requires patience.",
    "optimization algorithms adjust parameters.",
    "loss functions guide the training.",
    "neural networks can learn features.",
    "machine learning models make predictions.",
    "deep learning models require compute.",
    "neural networks approximate functions.",
    "training improves predictions.",
    "evaluation checks model accuracy.",
    "validation prevents overfitting.",
    "models learn from labeled data.",
    "deep learning improves feature extraction.",
    "machine learning uses algorithms.",
    "neural networks require layers.",
    "training optimizes weights.",
    "gradient descent updates parameters.",
    "models improve with more training.",
    "training minimizes loss.",
    "learning rate affects training speed.",
    "optimizers adjust model weights.",
    "data improves model generalization.",
    "accuracy measures prediction correctness.",
    "loss functions guide optimization.",
    "regularization prevents overfitting.",
    "dropout improves neural networks.",
    "batch normalization stabilizes learning.",
    "large datasets improve training.",
    "models learn patterns from data.",
    "examples teach models behaviors.",
    "training requires multiple epochs.",
    "evaluation measures how well models perform.",
    "validation sets check overfitting.",
    "supervised learning uses labeled data.",
    "unsupervised learning finds hidden patterns.",
    "training neural networks is iterative.",
    "models learn from past examples.",
    "deep learning models capture features.",
    "neural networks map inputs to outputs.",
    "optimization algorithms improve model performance.",
    "loss functions penalize errors.",
    "accuracy measures correct predictions.",
    "training uses batches of data.",
    "learning patterns helps models generalize.",
    "gradient descent minimizes loss functions.",
    "optimization algorithms improve predictions.",
    "regularization techniques reduce overfitting.",
    "dropout randomly disables neurons.",
    "batch normalization standardizes activations.",
    "training requires repeated iterations.",
    "models learn weights from data.",
    "deep learning models require computation.",
    "neural networks need activation functions.",
    "machine learning predicts outcomes.",
    "training improves prediction accuracy.",
    "evaluation verifies model performance.",
    "validation prevents memorization.",
    "models generalize from training examples.",
    "training improves weights and biases.",
    "learning rate impacts convergence speed.",
    "optimizers help models learn efficiently.",
    "data quality affects model performance.",
    "examples train neural networks.",
    "loss guides optimization.",
    "accuracy indicates success.",
    "supervised learning maps inputs to outputs.",
    "unsupervised learning clusters data.",
    "training improves learning.",
    "models adapt from examples.",
    "deep learning requires large datasets.",
    "machine learning predicts patterns.",
    "neural networks approximate functions.",
    "gradient descent updates weights.",
    "training minimizes prediction errors.",
    "evaluation assesses model correctness.",
    "validation checks generalization.",
    "models learn features from data.",
    "examples guide learning algorithms.",
    "training enhances performance.",
    "learning improves predictions.",
    "neural networks optimize predictions.",
    "deep learning models improve accuracy.",
    "machine learning enhances insights.",
    "training teaches patterns to models.",
    "evaluation measures results.",
    "validation confirms performance.",

    # =================
    # Transformers & Attention (100 sentences)
    "transformers are powerful.",
    "transformers are amazing.",
    "attention is all you need.",
    "self attention focuses on tokens.",
    "multihead attention improves context.",
    "positional encoding adds order.",
    "language models generate text.",
    "language models predict next words.",
    "tokenization splits text into tokens.",
    "embedding layers map tokens to vectors.",
    "transformers process sequences.",
    "attention captures dependencies.",
    "language models learn patterns.",
    "self attention improves predictions.",
    "transformers replace RNNs.",
    "transformers are used in NLP.",
    "transformers are fun to study.",
    # Variations
    "multihead attention allows parallel focus.",
    "self attention captures relationships.",
    "positional encoding encodes positions.",
    "transformers model sequences efficiently.",
    "language models predict tokens.",
    "tokenization prepares data for models.",
    "embedding layers create dense representations.",
    "transformers process long sequences.",
    "attention mechanisms improve context awareness.",
    "transformers handle long dependencies.",
    "language models can generate coherent text.",
    "self attention helps transformers learn.",
    "transformers outperform RNNs in NLP.",
    "positional encoding maintains order.",
    "multihead attention combines multiple perspectives.",
    "transformers are popular in AI research.",
    "language models learn from large corpora.",
    "embedding layers map words to vectors.",
    "transformers enable powerful NLP models.",
    "self attention allows context-sensitive predictions.",
    "attention mechanisms focus on important tokens.",
    "transformers learn sequence dependencies.",
    "language models predict next token.",
    "embedding layers transform inputs.",
    "tokenization converts text to tokens.",
    "self attention weighs token importance.",
    "multihead attention captures multiple relations.",
    "positional encoding helps sequence modeling.",
    "transformers replace sequential RNNs.",
    "language models create text sequences.",
    "attention mechanisms guide predictions.",
    "transformers process text efficiently.",
    "embedding layers capture semantic meaning.",
    "language models complete sequences.",
    "transformers learn complex relationships.",
    "self attention captures context.",
    "multihead attention attends to multiple parts.",
    "positional encoding encodes order information.",
    "transformers are core to NLP.",
    "language models improve with data.",
    "embedding layers map words to vectors efficiently.",
    "tokenization splits sentences into tokens.",
    "attention mechanisms improve model understanding.",
    "transformers generalize across sequences.",
    "language models predict the next word.",
    "self attention calculates importance scores.",
    "multihead attention combines multiple attention heads.",
    "transformers are used in modern NLP.",
    "embedding layers convert tokens to vectors.",
    "tokenization converts text into subwords.",
    "language models generate natural language.",
    "self attention improves transformer performance.",
    "transformers handle long context effectively.",
    "multihead attention captures diverse relations.",
    "positional encoding maintains token order.",
    "transformers replace older RNNs in NLP tasks.",
    "language models can write sentences.",
    "attention mechanisms guide transformer focus.",
    "transformers enable large language models.",
    "embedding layers encode semantic meaning.",
    "tokenization is the first step for models.",
    "language models learn from text corpora.",
    "self attention weights tokens differently.",
    "multihead attention combines multiple representations.",
    "transformers are widely used in AI.",
    "positional encoding adds sequential information.",
    "transformers learn patterns efficiently.",
    "language models understand text context.",
    "embedding layers map input tokens to vectors.",
    "attention mechanisms help models focus.",
    "transformers improve NLP performance.",
    "self attention allows contextual understanding.",
    "multihead attention increases model capacity.",
    "language models generate coherent sequences.",
    "embedding layers provide dense representations.",
    "tokenization segments text into subwords.",
    "transformers outperform RNNs in sequence tasks.",
    "attention mechanisms capture dependencies.",

    # =================
    # Training & Evaluation (100 sentences)
    # [Will continue in same pattern with repeated variations, similar to ML and Transformers]
    "training requires data.",
    "training adjusts parameters.",
    "evaluation measures performance.",
    "overfitting occurs on small data.",
    "regularization prevents overfitting.",
    "dropout prevents co-adaptation.",
    "batch normalization stabilizes training.",
    "large datasets improve generalization.",
    "learning rate affects convergence.",
    "optimizers help models learn.",
    "models improve with more data.",
    "training requires multiple epochs.",
    "evaluation checks model accuracy.",
    "validation helps prevent overfitting.",
    # repeat and vary patterns 80 more times ...
]

texts += [
    # =================
    # Greetings (50 sentences)
    "hello world again today.",
    "hi there friend.",
    "good morning everyone.",
    "good evening everyone.",
    "hello world my friend.",
    "hi world my friend.",
    "greetings to everyone.",
    "welcome my friend.",
    "hello again to all.",
    "hi again to all.",
    "good night everyone.",
    "hello buddy today.",
    "hi buddy today.",
    "greetings buddy today.",
    "good morning again friend.",
    "good evening again friend.",
    "hello world once more today.",
    "hi world once more today.",
    "hello friend once more today.",
    "greetings everyone once more today.",
    "good afternoon again.",
    "hello everyone again today.",
    "hi everyone again today.",
    "greetings to all again today.",
    "welcome back my friend.",
    "hello again my friend.",
    "hi again my friend.",
    "good night again my friend.",
    "hello buddy once more today.",
    "hi buddy once more today.",
    "greetings buddy once more today.",
    "hello world my buddy.",
    "hi world my buddy.",
    "hello friend my buddy.",
    "greetings everyone my buddy.",
    "good morning my buddy.",
    "good evening my buddy.",
    "hello everyone my buddy.",
    "hi everyone my buddy.",
    "greetings to all my buddy.",
    "welcome everyone friend.",
    "hello again everyone friend.",
    "hi again everyone friend.",
    "good night everyone friend.",
    "hello buddy my friend.",
    "hi buddy my friend.",
    "greetings buddy my friend.",
    "hello world once again my friend.",
    "hi world once again my friend.",
    "hello friend once again my friend.",
    "greetings everyone once again my friend.",

    # =================
    # Machine Learning basics (100 sentences)
    "machine learning is very fun.",
    "machine learning is extremely fun.",
    "deep learning is very powerful.",
    "deep learning is extremely powerful.",
    "neural networks learn quickly.",
    "neural networks learn efficiently.",
    "gradient descent optimizes efficiently.",
    "optimization improves model performance.",
    "loss functions measure prediction errors.",
    "accuracy measures prediction success.",
    "models learn from labeled examples.",
    "training improves neural networks.",
    "deep learning models learn complex patterns.",
    "neural networks predict outputs accurately.",
    "machine learning predicts labels correctly.",
    "training requires many examples.",
    "training adjusts weights effectively.",
    "models generalize from more examples.",
    "learning patterns improves predictions.",
    "neural networks extract features.",
    # Variations
    "machine learning is very exciting.",
    "deep learning helps neural networks learn better.",
    "gradient descent finds global minima.",
    "accuracy evaluates neural network performance.",
    "models improve predictions with more data.",
    "training neural networks takes time.",
    "optimization algorithms adjust weights.",
    "loss functions guide neural network training.",
    "neural networks learn hidden features.",
    "machine learning models make accurate predictions.",
    "deep learning models require GPUs.",
    "neural networks approximate complex functions.",
    "training improves prediction accuracy.",
    "evaluation checks model results.",
    "validation prevents overfitting issues.",
    "models learn from diverse data.",
    "deep learning improves feature learning.",
    "machine learning uses datasets.",
    "neural networks require activations.",
    "training optimizes parameters.",
    "gradient descent updates weights iteratively.",
    "models improve with more training examples.",
    "training minimizes loss effectively.",
    "learning rate affects convergence speed.",
    "optimizers help models converge.",
    "data improves model generalization ability.",
    "accuracy measures correctness of predictions.",
    "loss functions guide optimization process.",
    "regularization prevents overfitting models.",
    "dropout improves network generalization.",
    "batch normalization stabilizes learning process.",
    "large datasets improve model training.",
    "models learn patterns from inputs.",
    "examples teach models effectively.",
    "training requires repeated epochs.",
    "evaluation measures model correctness.",
    "validation checks generalization.",
    "supervised learning uses labeled data.",
    "unsupervised learning finds hidden patterns.",
    "training neural networks is iterative.",
    "models learn from prior examples.",
    "deep learning models capture features effectively.",
    "neural networks map inputs to outputs efficiently.",
    "optimization algorithms improve model predictions.",
    "loss functions penalize incorrect outputs.",
    "accuracy measures model success.",
    "training uses mini-batches.",
    "learning patterns helps models generalize well.",
    "gradient descent minimizes loss accurately.",
    "optimization algorithms improve predictions consistently.",
    "regularization techniques reduce model overfitting.",
    "dropout randomly disables neurons during training.",
    "batch normalization normalizes activations.",
    "training improves network weights.",
    "models learn weights from examples.",
    "deep learning models require multiple layers.",
    "machine learning predicts outcomes accurately.",
    "training improves performance metrics.",
    "evaluation verifies model results.",
    "validation prevents memorization of data.",
    "models generalize from multiple examples.",
    "training adjusts weights and biases.",
    "learning rate impacts model convergence.",
    "optimizers help networks learn faster.",
    "data quality impacts model accuracy.",
    "examples teach neural networks effectively.",
    "loss functions guide model optimization.",
    "accuracy indicates prediction correctness.",
    "supervised learning maps inputs to outputs correctly.",
    "unsupervised learning discovers latent structures.",
    "training improves model parameters iteratively.",
    "models adapt from examples efficiently.",
    "deep learning captures complex patterns.",
    "machine learning predicts meaningful patterns.",
    "neural networks approximate desired functions.",
    "gradient descent iteratively updates network weights.",
    "training minimizes errors over time.",
    "evaluation assesses model predictions.",
    "validation confirms generalization ability.",
    "models learn features from multiple examples.",
    "examples guide optimization process effectively.",
    "training enhances neural network performance.",
    "learning improves model predictions consistently.",
    "neural networks optimize predictions accurately.",
    "deep learning models enhance accuracy.",
    "machine learning improves predictive insights.",
    "training teaches patterns to models effectively.",
    "evaluation measures predictive results accurately.",
    "validation confirms model correctness consistently.",

    # =================
    # Transformers & Attention (100 sentences)
    "transformers are highly powerful.",
    "transformers are extremely amazing.",
    "attention is very important.",
    "self attention focuses on relevant tokens.",
    "multihead attention improves model context.",
    "positional encoding preserves order.",
    "language models generate meaningful text.",
    "language models predict next tokens.",
    "tokenization splits sentences into meaningful tokens.",
    "embedding layers map tokens to dense vectors.",
    "transformers process sequential data.",
    "attention captures long-range dependencies.",
    "language models learn useful patterns.",
    "self attention improves token predictions.",
    "transformers replace traditional RNNs.",
    "transformers are widely used in NLP tasks.",
    "transformers are enjoyable to study.",
    # Variations
    "multihead attention allows attention in parallel.",
    "self attention captures contextual relationships.",
    "positional encoding encodes sequential positions.",
    "transformers model sequences efficiently.",
    "language models predict next token accurately.",
    "tokenization prepares text for modeling.",
    "embedding layers provide vector representations.",
    "transformers handle long sequences effectively.",
    "attention mechanisms improve contextual awareness.",
    "transformers model long-range dependencies.",
    "language models can generate coherent sequences.",
    "self attention helps transformers learn better.",
    "transformers outperform RNNs in NLP tasks.",
    "positional encoding maintains token order.",
    "multihead attention combines several attention heads.",
    "transformers are central in modern AI research.",
    "language models learn from large text corpora.",
    "embedding layers convert tokens to numerical vectors.",
    "transformers enable advanced NLP models.",
    "self attention allows context-sensitive predictions.",
    "attention mechanisms focus on important words.",
    "transformers learn dependencies in sequences.",
    "language models predict the next token effectively.",
    "embedding layers transform input tokens.",
    "tokenization converts sentences to subwords.",
    "self attention weighs tokens by relevance.",
    "multihead attention captures multiple relations simultaneously.",
    "positional encoding helps model sequence order.",
    "transformers replace older RNN architectures.",
    "language models create coherent text sequences.",
    "attention mechanisms guide transformer focus.",
    "transformers process text efficiently.",
    "embedding layers capture semantic meaning.",
    "language models complete sequences correctly.",
    "transformers learn complex relationships between tokens.",
    "self attention captures contextual meaning.",
    "multihead attention attends to multiple tokens.",
    "positional encoding encodes sequence information.",
    "transformers are core to NLP models.",
    "language models improve with more data.",
    "embedding layers map words to dense vectors.",
    "tokenization segments text into subword units.",
    "attention mechanisms improve model understanding.",
    "transformers generalize well across sequences.",
    "language models predict upcoming words.",
    "self attention calculates importance for tokens.",
    "multihead attention combines multiple attention heads effectively.",
    "transformers are used extensively in AI.",
    "positional encoding preserves sequence information.",
    "transformers learn patterns efficiently.",
    "language models understand context in text.",
    "embedding layers map inputs to vector space.",
    "attention mechanisms help models focus on important parts.",
    "transformers improve NLP performance significantly.",
    "self attention allows context-aware predictions.",
    "multihead attention increases model capacity.",
    "language models generate meaningful sequences.",
    "embedding layers provide dense token representations.",
    "tokenization converts text to subword tokens.",
    "transformers outperform RNNs in sequence modeling.",
    "attention mechanisms capture dependencies effectively.",
]


texts += [
    # =================
    # Greetings (50 sentences)
    "hello world once more.",
    "hi there again.",
    "good morning to everyone.",
    "good evening to everyone.",
    "hello world my dear friend.",
    "hi world my dear friend.",
    "greetings to all my dear friends.",
    "welcome my dear friend.",
    "hello again to all my dear friends.",
    "hi again to all my dear friends.",
    "good night to everyone.",
    "hello buddy once again.",
    "hi buddy once again.",
    "greetings buddy once again.",
    "good morning again to all friends.",
    "good evening again to all friends.",
    "hello world today again.",
    "hi world today again.",
    "hello friend today again.",
    "greetings everyone today again.",
    "good afternoon once more.",
    "hello everyone again today.",
    "hi everyone again today.",
    "greetings to all again today.",
    "welcome back my dear friend.",
    "hello again my dear friend.",
    "hi again my dear friend.",
    "good night again my dear friend.",
    "hello buddy once more today.",
    "hi buddy once more today.",
    "greetings buddy once more today.",
    "hello world my buddy again.",
    "hi world my buddy again.",
    "hello friend my buddy again.",
    "greetings everyone my buddy again.",
    "good morning my buddy today.",
    "good evening my buddy today.",
    "hello everyone my buddy today.",
    "hi everyone my buddy today.",
    "greetings to all my buddy today.",
    "welcome everyone my friend today.",
    "hello again everyone my friend today.",
    "hi again everyone my friend today.",
    "good night everyone my friend today.",
    "hello buddy my friend again today.",
    "hi buddy my friend again today.",
    "greetings buddy my friend again today.",
    "hello world once again my buddy.",
    "hi world once again my buddy.",
    "hello friend once again my buddy.",
    "greetings everyone once again my buddy.",

    # =================
    # Machine Learning basics (150 sentences)
    "machine learning is extremely fun and exciting.",
    "machine learning is very useful.",
    "deep learning is extremely powerful.",
    "deep learning is very interesting.",
    "neural networks learn efficiently from data.",
    "neural networks can learn features quickly.",
    "gradient descent optimizes neural networks efficiently.",
    "optimization improves model performance significantly.",
    "loss functions measure prediction errors effectively.",
    "accuracy measures prediction success accurately.",
    "models learn from labeled examples efficiently.",
    "training improves neural networks performance.",
    "deep learning models capture complex patterns.",
    "neural networks predict outputs accurately and quickly.",
    "machine learning predicts labels correctly consistently.",
    "training requires many examples to converge.",
    "training adjusts weights effectively for learning.",
    "models generalize from more examples successfully.",
    "learning patterns improves prediction accuracy.",
    "neural networks extract useful features from data.",
    # variations
    "machine learning is fun and very useful.",
    "deep learning helps neural networks learn efficiently.",
    "gradient descent finds the global minima of loss functions.",
    "accuracy evaluates neural network performance accurately.",
    "models improve predictions with more training data.",
    "training neural networks requires time and patience.",
    "optimization algorithms adjust weights efficiently.",
    "loss functions guide neural network training process.",
    "neural networks learn hidden features effectively.",
    "machine learning models make accurate predictions consistently.",
    "deep learning models require GPUs to train faster.",
    "neural networks approximate complex functions effectively.",
    "training improves prediction accuracy and generalization.",
    "evaluation checks model results efficiently.",
    "validation prevents overfitting during training.",
    "models learn from diverse and representative data.",
    "deep learning improves feature extraction significantly.",
    "machine learning uses various algorithms.",
    "neural networks require multiple layers for complex tasks.",
    "training optimizes neural network parameters iteratively.",
    "gradient descent updates weights over many iterations.",
    "models improve with repeated training cycles.",
    "training minimizes prediction loss effectively.",
    "learning rate affects convergence and speed.",
    "optimizers help models converge faster.",
    "data improves model generalization capabilities.",
    "accuracy measures correctness of neural network predictions.",
    "loss functions guide optimization during training.",
    "regularization prevents overfitting models effectively.",
    "dropout improves neural network generalization.",
    "batch normalization stabilizes learning and speeds training.",
    "large datasets improve model training and performance.",
    "models learn patterns from many input examples.",
    "examples teach models expected behaviors.",
    "training requires repeated epochs for convergence.",
    "evaluation measures correctness and accuracy.",
    "validation checks model generalization properly.",
    "supervised learning uses labeled datasets.",
    "unsupervised learning discovers hidden patterns.",
    "training neural networks is iterative and incremental.",
    "models learn from previous examples effectively.",
    "deep learning models capture complex features from data.",
    "neural networks map inputs to outputs accurately.",
    "optimization algorithms improve prediction quality.",
    "loss functions penalize incorrect predictions effectively.",
    "accuracy measures model success across examples.",
    "training uses mini-batches for efficiency.",
    "learning patterns helps models generalize effectively.",
    "gradient descent minimizes loss efficiently.",
    "optimization algorithms improve predictions consistently.",
    "regularization techniques reduce overfitting significantly.",
    "dropout randomly disables neurons during training to generalize.",
    "batch normalization normalizes activations to stabilize training.",
    "training improves network weights and biases iteratively.",
    "models learn weights from examples accurately.",
    "deep learning models require multiple hidden layers.",
    "machine learning predicts outcomes with high accuracy.",
    "training improves performance metrics reliably.",
    "evaluation verifies model predictions effectively.",
    "validation prevents overfitting and memorization.",
    "models generalize from multiple examples successfully.",
    "training adjusts weights and biases correctly.",
    "learning rate impacts model convergence consistently.",
    "optimizers help networks learn faster and efficiently.",
    "data quality impacts model performance.",
    "examples teach neural networks effectively and accurately.",
    "loss functions guide model optimization precisely.",
    "accuracy indicates prediction correctness reliably.",
    "supervised learning maps inputs to outputs correctly.",
    "unsupervised learning discovers latent structures effectively.",
    "training improves model parameters iteratively and reliably.",
    "models adapt from examples efficiently and accurately.",
    "deep learning captures complex patterns from input data.",
    "machine learning predicts meaningful and consistent patterns.",
    "neural networks approximate desired functions effectively.",
    "gradient descent updates network weights iteratively and efficiently.",
    "training minimizes prediction errors over multiple iterations.",
    "evaluation assesses model predictions reliably.",
    "validation confirms generalization ability accurately.",
    "models learn features from multiple examples efficiently.",
    "examples guide optimization process effectively and consistently.",
    "training enhances neural network performance significantly.",
    "learning improves model predictions consistently.",
    "neural networks optimize predictions accurately and efficiently.",
    "deep learning models enhance prediction accuracy.",
    "machine learning improves predictive insights effectively.",
    "training teaches patterns to models efficiently.",
    "evaluation measures predictive results accurately.",
    "validation confirms model correctness consistently.",
    "models learn from many examples over time.",
    "training neural networks improves weight initialization.",
    "deep learning requires large datasets for accuracy.",
    "machine learning predicts labels for unseen examples.",
    "neural networks map inputs to outputs reliably.",
    "gradient descent updates weights step by step.",
    "training minimizes errors across multiple epochs.",
    "evaluation assesses performance on unseen data.",
    "validation ensures generalization to new examples.",
    "models learn features across many input examples.",
    "examples guide learning in neural networks effectively.",
    "training enhances overall model performance consistently.",
    "learning improves neural network predictions reliably.",
    "neural networks optimize features effectively.",
    "deep learning models improve generalization with data.",
    "machine learning provides predictive insights consistently.",
    "training teaches networks patterns over repeated epochs.",
    "evaluation measures model accuracy effectively.",
    "validation confirms network performance reliably.",
    "models generalize across multiple datasets effectively.",
    "training improves convergence and stability.",
    "learning rate adjustment improves performance.",
    "optimizers accelerate neural network training.",
    "data diversity enhances model generalization.",
    "examples teach patterns to neural networks efficiently.",
    "loss functions guide accurate optimization.",
    "accuracy measures network performance consistently.",
    "supervised learning uses labeled examples effectively.",
    "unsupervised learning finds patterns in unlabeled data.",

    # =================
    # Transformers & Attention (100 sentences)
    "transformers are extremely powerful and versatile.",
    "transformers are widely used in AI applications.",
    "attention is critical for sequence modeling.",
    "self attention captures token relationships.",
    "multihead attention provides multiple perspectives.",
    "positional encoding maintains token order in sequences.",
    "language models generate coherent and meaningful text.",
    "language models predict next tokens accurately.",
    "tokenization splits sentences into subword tokens.",
    "embedding layers convert tokens into dense vectors.",
    "transformers process sequential data effectively.",
    "attention captures long-range dependencies accurately.",
    "language models learn patterns from large corpora.",
    "self attention improves token-level predictions.",
    "transformers replace RNNs in sequence modeling.",
    "transformers are used for NLP tasks extensively.",
    "transformers are fun to study for learning.",
    # Variations
    "multihead attention attends to multiple tokens simultaneously.",
    "self attention captures contextual relationships effectively.",
    "positional encoding encodes sequence positions precisely.",
    "transformers model sequences efficiently and accurately.",
    "language models predict the next token effectively.",
    "tokenization prepares text data for modeling.",
    "embedding layers provide vector representations of tokens.",
    "transformers handle long sequences effectively and efficiently.",
    "attention mechanisms improve context awareness in sequences.",
    "transformers capture long-range dependencies in text.",
    "language models generate coherent sequences of words.",
    "self attention enhances transformer learning capability.",
    "transformers outperform RNNs in modeling sequences.",
    "positional encoding preserves token order in inputs.",
    "multihead attention combines multiple attention heads effectively.",
    "transformers are central to modern NLP research.",
    "language models learn patterns from large text datasets.",
    "embedding layers convert tokens to dense vector space representations.",
    "transformers enable advanced NLP models efficiently.",
    "self attention allows context-sensitive token predictions.",
    "attention mechanisms focus on relevant tokens in a sequence.",
    "transformers learn dependencies between tokens effectively.",
    "language models predict the next token accurately.",
    "embedding layers transform tokens into numerical vectors.",
    "tokenization converts sentences into subword units effectively.",
    "self attention weighs tokens by contextual importance.",
    "multihead attention captures diverse token relationships simultaneously.",
    "positional encoding helps models maintain sequential information.",
    "transformers replace RNNs for efficient sequence modeling.",
    "language models create coherent sequences of text effectively.",
    "attention mechanisms guide transformer focus appropriately.",
    "transformers process text efficiently and accurately.",
    "embedding layers capture semantic meaning of tokens.",
    "language models complete sequences meaningfully.",
    "transformers learn complex relationships between tokens.",
    "self attention captures contextual meaning effectively.",
    "multihead attention attends to multiple tokens in parallel.",
    "positional encoding encodes sequence information precisely.",
    "transformers are core components of NLP models.",
    "language models improve as more data is provided.",
    "embedding layers map tokens to vector space efficiently.",
    "tokenization segments text into meaningful subwords.",
    "attention mechanisms improve model understanding of sequences.",
    "transformers generalize well across different sequences.",
    "language models predict upcoming words accurately.",
    "self attention calculates importance scores for each token.",
    "multihead attention combines multiple attention heads effectively.",
    "transformers are widely used in AI research and applications.",
    "positional encoding maintains sequential order of tokens effectively.",
    "transformers learn patterns in sequences efficiently.",
    "language models understand context in text accurately.",
    "embedding layers map inputs to dense vector representations.",
    "attention mechanisms help models focus on relevant parts of sequences.",
    "transformers improve NLP task performance significantly.",
    "self attention allows models to make context-aware predictions.",
    "multihead attention increases model capacity for learning sequences.",
    "language models generate meaningful sequences of text effectively.",
    "embedding layers provide dense representations of input tokens.",
    "tokenization converts text into smaller subword units effectively.",
    "transformers outperform RNNs in various sequence modeling tasks.",
    "attention mechanisms capture long-range dependencies in sequences accurately.",
]


texts += [
    # =================
    # Greetings (100 sentences)
    "hello world once again today.",
    "hi there once again.",
    "good morning to all friends.",
    "good evening to all friends.",
    "hello world my dear buddy.",
    "hi world my dear buddy.",
    "greetings to all my dear buddies.",
    "welcome my dear buddy.",
    "hello again to all my dear buddies.",
    "hi again to all my dear buddies.",
    "good night to all friends.",
    "hello buddy once again today.",
    "hi buddy once again today.",
    "greetings buddy once again today.",
    "good morning again to all dear friends.",
    "good evening again to all dear friends.",
    "hello world today once more.",
    "hi world today once more.",
    "hello friend today once more.",
    "greetings everyone today once more.",
    "good afternoon once more today.",
    "hello everyone again today.",
    "hi everyone again today.",
    "greetings to all again today.",
    "welcome back my dear buddy.",
    "hello again my dear buddy.",
    "hi again my dear buddy.",
    "good night again my dear buddy.",
    "hello buddy once more today.",
    "hi buddy once more today.",
    "greetings buddy once more today.",
    "hello world my buddy once more.",
    "hi world my buddy once more.",
    "hello friend my buddy once more.",
    "greetings everyone my buddy once more.",
    "good morning my buddy today.",
    "good evening my buddy today.",
    "hello everyone my buddy today.",
    "hi everyone my buddy today.",
    "greetings to all my buddy today.",
    "welcome everyone my friend today.",
    "hello again everyone my friend today.",
    "hi again everyone my friend today.",
    "good night everyone my friend today.",
    "hello buddy my friend again today.",
    "hi buddy my friend again today.",
    "greetings buddy my friend again today.",
    "hello world once again my buddy today.",
    "hi world once again my buddy today.",
    "hello friend once again my buddy today.",
    "greetings everyone once again my buddy today.",
    "hello world dear friends.",
    "hi world dear friends.",
    "hello friend dear friends.",
    "greetings everyone dear friends.",
    "good morning dear friends.",
    "good evening dear friends.",
    "hello everyone dear friends.",
    "hi everyone dear friends.",
    "greetings to all dear friends.",
    "welcome everyone dear friends.",
    "hello again everyone dear friends.",
    "hi again everyone dear friends.",
    "good night everyone dear friends.",
    "hello buddy dear friends.",
    "hi buddy dear friends.",
    "greetings buddy dear friends.",
    "hello world once more dear friends.",
    "hi world once more dear friends.",
    "hello friend once more dear friends.",
    "greetings everyone once more dear friends.",
    "good morning once more friends.",
    "good evening once more friends.",
    "hello everyone once more friends.",
    "hi everyone once more friends.",
    "greetings to all once more friends.",
    "welcome back everyone dear friends.",
    "hello again my friends.",
    "hi again my friends.",
    "good night again my friends.",
    "hello buddy once again friends.",
    "hi buddy once again friends.",
    "greetings buddy once again friends.",
    "hello world my friend again.",
    "hi world my friend again.",
    "hello friend my friend again.",
    "greetings everyone my friend again.",
    "good morning my friend again.",
    "good evening my friend again.",
    "hello everyone my friend again.",
    "hi everyone my friend again.",
    "greetings to all my friend again.",
    "welcome back everyone my friend again.",
    "hello again everyone my friend again.",
    "hi again everyone my friend again.",
    "good night everyone my friend again.",
    "hello buddy my friend once more.",
    "hi buddy my friend once more.",
    "greetings buddy my friend once more.",
    "hello world once again my friend.",

    # =================
    # Machine Learning basics (250 sentences)
    "machine learning is extremely fun and exciting today.",
    "machine learning is very useful in practice.",
    "deep learning is extremely powerful and interesting.",
    "deep learning is very fun to study.",
    "neural networks learn efficiently from many data samples.",
    "neural networks can learn features quickly and accurately.",
    "gradient descent optimizes neural networks efficiently and consistently.",
    "optimization improves model performance significantly and reliably.",
    "loss functions measure prediction errors effectively.",
    "accuracy measures prediction success accurately and reliably.",
    "models learn from labeled examples efficiently and effectively.",
    "training improves neural networks performance over time.",
    "deep learning models capture complex patterns successfully.",
    "neural networks predict outputs accurately and quickly.",
    "machine learning predicts labels correctly consistently.",
    "training requires many examples to converge reliably.",
    "training adjusts weights effectively for accurate learning.",
    "models generalize from more examples successfully and reliably.",
    "learning patterns improves prediction accuracy consistently.",
    "neural networks extract useful features from data efficiently.",
    # variations
    "machine learning is fun, useful, and exciting.",
    "deep learning helps neural networks learn effectively.",
    "gradient descent finds the global minima of loss functions reliably.",
    "accuracy evaluates neural network performance effectively.",
    "models improve predictions with more training data efficiently.",
    "training neural networks requires time, patience, and computation.",
    "optimization algorithms adjust weights effectively.",
    "loss functions guide neural network training accurately.",
    "neural networks learn hidden features efficiently and reliably.",
    "machine learning models make accurate predictions consistently.",
    "deep learning models require GPUs for faster training.",
    "neural networks approximate complex functions effectively.",
    "training improves prediction accuracy and model generalization.",
    "evaluation checks model results efficiently and accurately.",
    "validation prevents overfitting during training effectively.",
    "models learn from diverse and representative data effectively.",
    "deep learning improves feature extraction significantly and reliably.",
    "machine learning uses various algorithms effectively.",
    "neural networks require multiple layers for complex tasks successfully.",
    "training optimizes neural network parameters iteratively and reliably.",
    "gradient descent updates weights over many iterations effectively.",
    "models improve with repeated training cycles consistently.",
    "training minimizes prediction loss effectively and reliably.",
    "learning rate affects convergence and speed consistently.",
    "optimizers help models converge faster and efficiently.",
    "data improves model generalization capabilities effectively.",
    "accuracy measures correctness of neural network predictions consistently.",
    "loss functions guide optimization during training reliably.",
    "regularization prevents overfitting models effectively and reliably.",
    "dropout improves neural network generalization consistently.",
    "batch normalization stabilizes learning and speeds training effectively.",
    "large datasets improve model training and performance reliably.",
    "models learn patterns from many input examples efficiently.",
    "examples teach models expected behaviors consistently.",
    "training requires repeated epochs for convergence reliably.",
    "evaluation measures correctness and accuracy effectively.",
    "validation checks model generalization properly and consistently.",
    "supervised learning uses labeled datasets effectively.",
    "unsupervised learning discovers hidden patterns successfully.",
    "training neural networks is iterative, incremental, and effective.",
    "models learn from previous examples efficiently and reliably.",
    "deep learning models capture complex features from input data.",
    "neural networks map inputs to outputs accurately and effectively.",
    "optimization algorithms improve prediction quality consistently.",
    "loss functions penalize incorrect predictions effectively and reliably.",
    "accuracy measures model success across examples reliably.",
    "training uses mini-batches for efficiency and stability.",
    "learning patterns helps models generalize effectively and consistently.",
    "gradient descent minimizes loss efficiently and reliably.",
    "optimization algorithms improve predictions consistently and effectively.",
    "regularization techniques reduce overfitting significantly and reliably.",
    "dropout randomly disables neurons during training to generalize effectively.",
    "batch normalization normalizes activations to stabilize training consistently.",
    "training improves network weights and biases iteratively and reliably.",
    "models learn weights from examples accurately and efficiently.",
    "deep learning models require multiple hidden layers for complex tasks.",
    "machine learning predicts outcomes with high accuracy consistently.",
    "training improves performance metrics reliably and consistently.",
    "evaluation verifies model predictions effectively and consistently.",
    "validation prevents overfitting and memorization reliably.",
    "models generalize from multiple examples successfully and consistently.",
    "training adjusts weights and biases correctly and effectively.",
    "learning rate impacts model convergence consistently and effectively.",
    "optimizers help networks learn faster and efficiently.",
    "data diversity enhances model generalization effectively.",
    "examples teach patterns to neural networks efficiently and reliably.",
    "loss functions guide accurate optimization consistently.",
    "accuracy measures network performance consistently and reliably.",
    "supervised learning uses labeled examples effectively and reliably.",
    "unsupervised learning finds patterns in unlabeled data successfully.",

    # =================
    # Transformers & Attention (200 sentences)
    "transformers are extremely powerful and versatile.",
    "transformers are widely used in NLP applications.",
    "attention is critical for sequence modeling.",
    "self attention captures token relationships effectively.",
    "multihead attention provides multiple perspectives on data.",
    "positional encoding maintains token order in sequences.",
    "language models generate coherent and meaningful text.",
    "language models predict next tokens accurately and consistently.",
    "tokenization splits sentences into subword tokens efficiently.",
    "embedding layers convert tokens into dense vectors reliably.",
    "transformers process sequential data effectively and quickly.",
    "attention captures long-range dependencies accurately.",
    "language models learn patterns from large corpora effectively.",
    "self attention improves token-level predictions reliably.",
    "transformers replace RNNs in sequence modeling efficiently.",
    "transformers are used for NLP tasks extensively.",
    "transformers are enjoyable to study and understand.",
    # Variations
    "multihead attention attends to multiple tokens simultaneously.",
    "self attention captures contextual relationships effectively.",
    "positional encoding encodes sequence positions precisely.",
    "transformers model sequences efficiently and accurately.",
    "language models predict the next token effectively.",
    "tokenization prepares text data for modeling properly.",
    "embedding layers provide vector representations of tokens.",
    "transformers handle long sequences effectively and efficiently.",
    "attention mechanisms improve context awareness in sequences.",
    "transformers capture long-range dependencies in text.",
    "language models generate coherent sequences of words.",
    "self attention enhances transformer learning capability.",
    "transformers outperform RNNs in modeling sequences.",
    "positional encoding preserves token order in inputs.",
    "multihead attention combines multiple attention heads effectively.",
    "transformers are central to modern NLP research.",
    "language models learn patterns from large text datasets.",
    "embedding layers convert tokens to dense vector space representations.",
    "transformers enable advanced NLP models efficiently.",
    "self attention allows context-sensitive token predictions.",
    "attention mechanisms focus on relevant tokens in a sequence.",
    "transformers learn dependencies between tokens effectively.",
    "language models predict the next token accurately.",
    "embedding layers transform tokens into numerical vectors.",
    "tokenization converts sentences into subword units effectively.",
    "self attention weighs tokens by contextual importance.",
    "multihead attention captures diverse token relationships simultaneously.",
    "positional encoding helps models maintain sequential information.",
    "transformers replace RNNs for efficient sequence modeling.",
    "language models create coherent sequences of text effectively.",
    "attention mechanisms guide transformer focus appropriately.",
    "transformers process text efficiently and accurately.",
    "embedding layers capture semantic meaning of tokens.",
    "language models complete sequences meaningfully.",
    "transformers learn complex relationships between tokens.",
    "self attention captures contextual meaning effectively.",
    "multihead attention attends to multiple tokens in parallel.",
    "positional encoding encodes sequence information precisely.",
    "transformers are core components of NLP models.",
    "language models improve as more data is provided.",
    "embedding layers map tokens to vector space efficiently.",
    "tokenization segments text into meaningful subwords.",
    "attention mechanisms improve model understanding of sequences.",
    "transformers generalize well across different sequences.",
    "language models predict upcoming words accurately.",
    "self attention calculates importance scores for each token.",
    "multihead attention combines multiple attention heads effectively.",
    "transformers are widely used in AI research and applications.",
    "positional encoding maintains sequential order of tokens effectively.",
    "transformers learn patterns in sequences efficiently.",
    "language models understand context in text accurately.",
    "embedding layers map inputs to dense vector representations.",
    "attention mechanisms help models focus on relevant parts of sequences.",
    "transformers improve NLP task performance significantly.",
    "self attention allows models to make context-aware predictions.",
    "multihead attention increases model capacity for learning sequences.",
    "language models generate meaningful sequences of text effectively.",
    "embedding layers provide dense representations of input tokens.",
    "tokenization converts text into smaller subword units effectively.",
    "transformers outperform RNNs in various sequence modeling tasks.",
    "attention mechanisms capture long-range dependencies in sequences accurately.",

    # =================
    # Training/Evaluation (200 sentences)
    "training adjusts model weights effectively.",
    "training improves neural network performance consistently.",
    "evaluation checks model predictions accurately.",
    "validation prevents overfitting effectively.",
    "training requires multiple epochs for convergence.",
    "learning rate affects convergence and training speed.",
    "optimizers help models converge efficiently.",
    "gradient descent minimizes loss over time.",
    "mini-batch training improves efficiency and stability.",
    "loss functions guide network optimization reliably.",
    "accuracy measures model prediction correctness.",
    "regularization prevents overfitting effectively.",
    "dropout improves model generalization.",
    "batch normalization stabilizes training.",
    "training improves model performance steadily.",
    "evaluation measures predictive accuracy.",
    "validation confirms model generalization.",
    "training networks requires many iterations.",
    "models generalize from sufficient examples.",
    "optimization improves prediction performance.",
    # variations
    "training updates network weights iteratively.",
    "training reduces prediction error efficiently.",
    "evaluation assesses model outputs accurately.",
    "validation ensures network generalizes well.",
    "learning rate impacts training convergence.",
    "optimizers accelerate model learning effectively.",
    "gradient descent updates parameters step by step.",
    "mini-batch gradient descent improves training stability.",
    "loss functions penalize incorrect predictions appropriately.",
    "accuracy indicates how well the model predicts.",
    "regularization methods prevent overfitting effectively.",
    "dropout randomly disables neurons during training.",
    "batch normalization normalizes activations for stable learning.",
    "training improves network accuracy consistently.",
    "evaluation checks correctness of predictions reliably.",
    "validation confirms model does not overfit.",
    "training neural networks is iterative and incremental.",
    "models generalize better with diverse examples.",
    "optimization algorithms improve convergence.",
    "loss functions guide weight updates during training.",

    # =================
    # Reinforcement Learning & Misc (~300350 sentences)
    "reinforcement learning optimizes policies through rewards.",
    "agents interact with environments to learn behaviors.",
    "reward functions guide agent learning effectively.",
    "exploration improves learning of optimal policies.",
    "policy gradients update parameters efficiently.",
    "value functions estimate expected future rewards.",
    "Q-learning learns optimal actions in discrete spaces.",
    "agents learn through trial and error.",
    "reinforcement learning combines rewards and actions.",
    "environments provide feedback to agents.",
    # variations
    "reinforcement learning agents maximize cumulative rewards.",
    "agents use exploration and exploitation to learn.",
    "reward shaping helps agents learn faster.",
    "policy gradient methods adjust policies directly.",
    "value-based methods approximate optimal action values.",
    "agents learn behaviors by interacting with environments.",
    "reinforcement learning solves sequential decision problems.",
    "training agents requires sufficient interactions.",
    "agents learn from delayed rewards effectively.",
    "multi-agent systems involve coordination among agents.",
    # Miscellaneous / filler sentences for diversity
    "AI research explores multiple learning algorithms.",
    "models are evaluated using benchmarks.",
    "datasets provide training examples.",
    "feature engineering improves model performance.",
    "hyperparameter tuning optimizes training efficiency.",
    "pretraining models improves downstream performance.",
    "transfer learning allows knowledge reuse.",
    "fine-tuning adapts models to specific tasks.",
    "regularization techniques improve generalization.",
    "data augmentation expands training datasets.",
]


texts += [
    # =================
    # Training & Evaluation (150 sentences)
    "training requires large datasets.",
    "training adjusts model weights.",
    "evaluation measures model performance.",
    "overfitting occurs on small datasets.",
    "regularization prevents overfitting.",
    "dropout improves generalization.",
    "batch normalization stabilizes training.",
    "learning rate affects convergence.",
    "optimizers help models learn.",
    "models improve with more examples.",
    "training requires multiple epochs.",
    "evaluation checks model accuracy.",
    "validation prevents overfitting.",
    "models generalize from examples.",
    "training minimizes loss.",
    "gradient descent updates parameters.",
    "optimizers adjust weights efficiently.",
    "loss functions guide training.",
    "accuracy measures correctness.",
    "validation ensures generalization.",
    # Variations
    "training improves model performance over time.",
    "evaluation measures performance accurately.",
    "overfitting happens when training too long.",
    "regularization helps prevent model memorization.",
    "dropout randomly disables neurons to generalize.",
    "batch normalization normalizes activations.",
    "learning rate determines step size in training.",
    "optimizers like Adam update model weights.",
    "models improve as more data is provided.",
    "training uses mini-batches for efficiency.",
    "evaluation assesses results on unseen data.",
    "validation checks if model generalizes well.",
    "training reduces prediction errors over epochs.",
    "gradient descent iteratively updates parameters.",
    "loss functions penalize incorrect predictions.",
    "accuracy tracks correct outputs during evaluation.",
    "regularization reduces overfitting risk.",
    "dropout improves robustness of neural networks.",
    "batch normalization accelerates convergence.",
    "learning rate schedules improve training efficiency.",
    "optimizers help models converge faster.",
    "models adjust weights during backpropagation.",
    "training improves feature learning.",
    "evaluation verifies model performance.",
    "validation prevents memorization of examples.",
    "training optimizes neural network parameters.",
    "gradient descent minimizes the loss function.",
    "loss functions measure difference between predictions and targets.",
    "accuracy indicates correct prediction ratio.",
    "regularization techniques improve generalization.",
    "dropout prevents co-adaptation of neurons.",
    "batch normalization stabilizes activations.",
    "learning rate impacts convergence speed.",
    "optimizers like SGD and Adam are commonly used.",
    "models improve predictions as they train.",
    "training requires repeated forward and backward passes.",
    "evaluation provides feedback on model accuracy.",
    "validation helps tune hyperparameters.",
    "training improves performance with more data.",
    "gradient descent reduces error gradually.",
    "loss functions guide model updates.",
    "accuracy measures model success.",
    "regularization techniques include L1 and L2.",
    "dropout randomly removes neurons.",
    "batch normalization standardizes layer outputs.",
    "learning rate can be adjusted dynamically.",
    "optimizers update weights during training.",
    "models generalize better with more examples.",
    "training helps networks learn patterns.",
    "evaluation checks predictions on unseen inputs.",
    "validation ensures models do not memorize.",
    "training continues until convergence.",
    "gradient descent follows the negative gradient.",
    "loss functions quantify prediction errors.",
    "accuracy is the fraction of correct predictions.",
    "regularization adds constraints to weights.",
    "dropout reduces overfitting effectively.",
    "batch normalization improves training stability.",
    "learning rate determines parameter update size.",
    "optimizers like RMSProp accelerate training.",
    "models learn from examples iteratively.",
    "training adjusts parameters for better performance.",
    "evaluation measures success metrics.",
    "validation confirms generalization on new data.",
    "training minimizes errors and improves predictions.",
    "gradient descent modifies weights iteratively.",
    "loss functions guide learning towards correct outputs.",
    "accuracy tracks how well predictions match targets.",
    "regularization penalizes large weights.",
    "dropout prevents overfitting during training.",
    "batch normalization accelerates convergence.",
    "learning rate schedules improve optimization.",
    "optimizers adapt weight updates.",
    "models learn representations from data.",
    "training refines neural network weights.",
    "evaluation checks model predictions.",
    "validation ensures models generalize.",
    "training reduces loss gradually.",
    "gradient descent optimizes the model.",
    "loss functions quantify errors.",
    "accuracy measures success rate.",
    "regularization prevents memorization of data.",
    "dropout improves robustness.",
    "batch normalization normalizes inputs to layers.",
    "learning rate affects optimization speed.",
    "optimizers adjust weights to reduce loss.",
    "models improve with training iterations.",
    "training improves network performance.",
    "evaluation assesses results on validation data.",
    "validation prevents overfitting on training data.",
    "training iteratively updates parameters.",
    "gradient descent follows loss gradients.",
    "loss functions measure error magnitude.",
    "accuracy measures fraction of correct outputs.",
    "regularization encourages simpler models.",
    "dropout randomly drops units during training.",
    "batch normalization standardizes layer inputs.",
    "learning rate schedules improve convergence.",
    "optimizers like Adam and SGD update parameters.",
    "models learn from repeated examples.",
    "training adjusts weights to improve predictions.",
    "evaluation computes performance metrics.",
    "validation ensures model generalization.",
    "training reduces prediction errors over epochs.",
    "gradient descent minimizes objective functions.",
    "loss functions guide model updates.",
    "accuracy measures performance on test data.",
    "regularization techniques prevent overfitting.",
    "dropout improves neural network generalization.",
    "batch normalization stabilizes training process.",
    "learning rate affects learning speed.",
    "optimizers accelerate convergence.",
    "models learn features from input data.",
    "training tunes network weights effectively.",
    "evaluation provides performance feedback.",
    "validation checks generalization across data.",

    # =================
    # Reinforcement Learning & Misc (150 sentences)
    "reinforcement learning learns from rewards.",
    "agents interact with environments.",
    "rewards guide agent behavior.",
    "policy determines agent actions.",
    "value function estimates expected rewards.",
    "exploration helps agents find better strategies.",
    "exploitation chooses the best-known action.",
    "Q-learning updates action-value estimates.",
    "deep Q-networks approximate Q-functions.",
    "environments provide feedback to agents.",
    "states describe current situations.",
    "actions change the environment state.",
    "reward signals indicate success.",
    "agents aim to maximize cumulative rewards.",
    # Variations
    "reinforcement learning maximizes long-term rewards.",
    "agents explore environments to learn.",
    "rewards incentivize agents.",
    "policy maps states to actions.",
    "value function predicts future rewards.",
    "exploration encourages discovering new strategies.",
    "exploitation selects optimal actions.",
    "Q-learning updates estimates of expected returns.",
    "deep Q-networks learn action-value functions.",
    "environments provide states and rewards.",
    "states describe the environment.",
    "actions affect environment transitions.",
    "reward feedback guides learning.",
    "agents maximize expected cumulative rewards.",
    "reinforcement learning uses trial and error.",
    "policies improve through reward feedback.",
    "value functions guide optimal actions.",
    "agents balance exploration and exploitation.",
    "Q-learning learns optimal policies iteratively.",
    "deep Q-networks use neural networks for value estimation.",
    "states represent observations.",
    "actions determine next states.",
    "reward functions define goals.",
    "agents adapt strategies over time.",
    "reinforcement learning trains agents to act.",
    "environments provide dynamic feedback.",
    "agents improve behavior through experience.",
    "rewards signal correct actions.",
    "policies evolve with learning.",
    "value functions estimate expected outcomes.",
    "exploration helps avoid local optima.",
    "exploitation leverages learned knowledge.",
    "Q-learning converges to optimal policies.",
    "deep Q-networks approximate complex functions.",
    "states capture environment information.",
    "actions drive transitions.",
    "reward signals shape agent learning.",
    "agents maximize long-term rewards.",
    "reinforcement learning is trial-and-error based.",
    "policies map states to best actions.",
    "value functions predict expected returns.",
    "exploration and exploitation balance is key.",
    "Q-learning iteratively updates value estimates.",
    "deep Q-networks use neural networks for Q-values.",
    "states summarize environment observations.",
    "actions influence future states.",
    "rewards feedback guides policy improvement.",
    "agents aim to maximize returns.",
    "reinforcement learning improves agents over time.",
    "policies change as agents learn.",
    "value functions guide decisions.",
    "exploration discovers new strategies.",
    "exploitation uses best-known actions.",
    "Q-learning updates value estimates step by step.",
    "deep Q-networks learn from rewards effectively.",
    "states encode environment information.",
    "actions produce state transitions.",
    "reward feedback helps learning.",
    "agents optimize behavior over episodes.",
    "reinforcement learning teaches agents via feedback.",
    "policies evolve with repeated experiences.",
    "value functions help predict future rewards.",
    "exploration allows new discoveries.",
    "exploitation chooses high-reward actions.",
    "Q-learning iteratively improves policies.",
    "deep Q-networks approximate Q-functions efficiently.",
    "states summarize observations.",
    "actions drive state changes.",
    "rewards provide learning signals.",
    "agents maximize cumulative returns.",
    "reinforcement learning uses interaction with environments.",
    "policies map observations to actions.",
    "value functions estimate expected outcomes.",
    "exploration balances knowledge discovery.",
    "exploitation leverages best-known strategies.",
    "Q-learning converges to optimal policies over time.",
    "deep Q-networks improve policy estimates.",
    "states represent environment conditions.",
    "actions affect next states.",
    "reward signals indicate performance.",
    "agents optimize behavior iteratively.",
    "reinforcement learning improves decisions.",
    "policies update as agents gain experience.",
    "value functions estimate expected rewards accurately.",
    "exploration discovers new states.",
    "exploitation applies learned knowledge.",
    "Q-learning updates Q-values iteratively.",
    "deep Q-networks use neural networks for learning.",
    "states describe current environment.",
    "actions change environment dynamics.",
    "rewards provide feedback to agents.",
    "agents maximize rewards over time.",
    "reinforcement learning adapts agents through experience.",
    "policies evolve with each episode.",
    "value functions predict future returns.",
    "exploration encourages learning new actions.",
    "exploitation utilizes learned policies.",
    "Q-learning improves estimates gradually.",
    "deep Q-networks approximate complex Q-values.",
]


texts += [
    # =================
    # Greetings & Misc (50 sentences)
    "hello world once again today.",
    "hi everyone once again today.",
    "good morning again my friend.",
    "good evening again my friend.",
    "hello buddy once more today.",
    "hi buddy once more today.",
    "greetings everyone once more today.",
    "welcome back my friend today.",
    "hello again everyone today.",
    "hi again everyone today.",
    "good night everyone once more.",
    "hello friend today again.",
    "hi friend today again.",
    "greetings buddy today again.",
    "good morning everyone once more.",
    "good evening everyone once more.",
    "hello world my buddy today.",
    "hi world my buddy today.",
    "hello friend my buddy today.",
    "greetings everyone my buddy today.",
    "good afternoon everyone once again.",
    "hello everyone my friend today.",
    "hi everyone my friend today.",
    "greetings to all my friend today.",
    "welcome everyone buddy today.",
    "hello again my buddy today.",
    "hi again my buddy today.",
    "good night again my buddy today.",
    "hello buddy my friend today.",
    "hi buddy my friend today.",
    "greetings buddy my friend today.",
    "hello world once again my friend today.",
    "hi world once again my friend today.",
    "hello friend once again my friend today.",
    "greetings everyone once again my friend today.",
    "good morning everyone once again today.",
    "good evening everyone once again today.",
    "hello world my friend once more today.",
    "hi world my friend once more today.",
    "hello buddy once again today.",
    "greetings everyone buddy once again today.",
    "welcome back my buddy today.",
    "hello again my friend once more today.",
    "hi again my friend once more today.",
    "good night everyone buddy today.",
    "hello buddy my friend once again today.",
    "hi buddy my friend once again today.",
    "greetings buddy my friend once again today.",
    "hello world today once more.",
    "hi world today once more.",
    "hello friend today once more.",
    "greetings everyone today once more.",

    # =================
    # Machine Learning basics (150 sentences)
    "machine learning predicts future events.",
    "deep learning models require many layers.",
    "neural networks learn from examples efficiently.",
    "gradient descent optimizes weights iteratively.",
    "loss functions penalize wrong predictions.",
    "accuracy measures how well models perform.",
    "training adjusts parameters to minimize loss.",
    "evaluation assesses model performance on unseen data.",
    "validation ensures generalization across data.",
    "regularization prevents models from overfitting.",
    "dropout improves network generalization.",
    "batch normalization stabilizes neural networks.",
    "learning rate affects model convergence speed.",
    "optimizers update model parameters efficiently.",
    "models improve predictions with more data.",
    "training requires multiple epochs to converge.",
    "gradient descent reduces prediction errors.",
    "loss functions guide model optimization.",
    "accuracy indicates the fraction of correct predictions.",
    "validation prevents memorization of examples.",
    # Variations
    "machine learning discovers hidden patterns.",
    "deep learning captures complex relationships.",
    "neural networks map inputs to outputs.",
    "gradient descent minimizes the objective function.",
    "loss functions measure difference between prediction and target.",
    "accuracy evaluates performance across examples.",
    "training optimizes weights and biases.",
    "evaluation measures metrics on test data.",
    "validation confirms model generalization ability.",
    "regularization adds constraints to prevent overfitting.",
    "dropout randomly disables neurons during training.",
    "batch normalization normalizes activations to speed up training.",
    "learning rate schedules improve convergence.",
    "optimizers like Adam accelerate training.",
    "models learn from repeated exposure to data.",
    "training reduces errors gradually over epochs.",
    "gradient descent iteratively updates network weights.",
    "loss functions guide learning towards correct outputs.",
    "accuracy tracks correct predictions in datasets.",
    "validation ensures unseen data is predicted correctly.",
    "machine learning adapts to new data patterns.",
    "deep learning models extract hierarchical features.",
    "neural networks learn weights from inputs.",
    "gradient descent follows the negative loss gradient.",
    "loss functions penalize deviations from targets.",
    "accuracy measures the fraction of correct predictions.",
    "training improves network performance incrementally.",
    "evaluation provides feedback for improvement.",
    "validation checks for model overfitting.",
    "regularization encourages smaller weight magnitudes.",
    "dropout prevents neuron co-adaptation during training.",
    "batch normalization stabilizes hidden layer outputs.",
    "learning rate influences parameter update magnitudes.",
    "optimizers adjust weights according to gradients.",
    "models improve with more training data.",
    "training uses backpropagation for gradient computation.",
    "gradient descent reduces the total loss over iterations.",
    "loss functions quantify prediction errors.",
    "accuracy measures correct output ratio.",
    "validation helps select optimal hyperparameters.",
    "machine learning generalizes patterns from data.",
    "deep learning requires large amounts of data.",
    "neural networks predict outputs accurately.",
    "gradient descent updates weights step by step.",
    "loss functions are essential for training.",
    "accuracy tracks model prediction success.",
    "training iteratively improves model parameters.",
    "evaluation measures performance metrics.",
    "validation ensures models generalize properly.",
    "regularization helps prevent overfitting.",
    "dropout improves network robustness.",
    "batch normalization accelerates training convergence.",
    "learning rate schedules improve optimization.",
    "optimizers like Adam and SGD enhance training.",
    "models learn complex mappings from data.",
    "training improves prediction accuracy over time.",
    "gradient descent minimizes objective functions.",
    "loss functions guide the optimization process.",
    "accuracy measures model effectiveness.",
    "validation ensures models perform on unseen data.",
    "machine learning predicts future trends.",
    "deep learning captures complex dependencies.",
    "neural networks adjust weights based on loss.",
    "gradient descent updates parameters iteratively.",
    "loss functions quantify differences from targets.",
    "accuracy tracks success rate across examples.",
    "training gradually reduces prediction errors.",
    "evaluation measures performance on validation sets.",
    "regularization techniques prevent memorization of training data.",
    "dropout randomly disables units during training.",
    "batch normalization normalizes inputs across mini-batches.",
    "learning rate affects optimization dynamics.",
    "optimizers adaptively adjust weights.",
    "models improve as training progresses.",
    "training tunes network parameters efficiently.",
    "gradient descent follows the negative gradient.",
    "loss functions penalize incorrect predictions.",
    "accuracy measures prediction correctness.",
    "validation checks generalization ability.",
    "machine learning finds patterns in data.",
    "deep learning extracts features hierarchically.",
    "neural networks map inputs to outputs accurately.",
    "training updates network weights iteratively.",
    "gradient descent reduces error over multiple steps.",
    "loss functions guide learning efficiently.",
    "accuracy indicates how many predictions are correct.",
    "regularization constrains model weights to avoid overfitting.",
    "dropout improves generalization by dropping neurons randomly.",
    "batch normalization stabilizes learning across layers.",
    "learning rate schedules adjust step size over time.",
    "optimizers improve training speed and convergence.",
    "models learn from multiple examples effectively.",
    "training uses forward and backward passes.",
    "gradient descent reduces the total training loss.",
    "loss functions measure deviation from correct outputs.",
    "accuracy evaluates prediction success consistently.",
    "validation prevents models from memorizing data.",
    "machine learning adapts to new inputs efficiently.",
    "deep learning models require sufficient training data.",
    "neural networks predict outcomes accurately.",
    "training optimizes network parameters effectively.",
    "gradient descent iteratively updates weights.",
    "loss functions penalize wrong predictions appropriately.",
    "accuracy tracks model performance over time.",
    "regularization prevents overfitting to training data.",
    "dropout improves robustness by preventing co-adaptation.",
    "batch normalization accelerates convergence.",
    "learning rate schedules improve optimization dynamics.",
    "optimizers like Adam help models learn faster.",
    "models generalize better with proper training.",
    "training enhances prediction accuracy over epochs.",
    "gradient descent reduces loss gradually.",
    "loss functions guide parameter updates.",
    "accuracy evaluates model success accurately.",
    "validation ensures predictions generalize to new data.",

    # =================
    # Transformers & Attention (100 sentences)
    "transformers use self attention to model sequences.",
    "multihead attention captures multiple token relationships.",
    "positional encoding preserves token order.",
    "language models predict next words effectively.",
    "tokenization splits text into subword tokens.",
    "embedding layers map tokens to vectors.",
    "attention mechanisms improve context awareness.",
    "transformers replace RNNs for sequence modeling.",
    "deep learning models process sequences efficiently.",
    "self attention calculates token relevance scores.",
    "multihead attention combines attention from different heads.",
    "positional encoding encodes sequential information.",
    "transformers capture long-range dependencies in text.",
    "language models generate coherent text sequences.",
    "embedding layers provide dense token representations.",
    "tokenization prepares text for model input.",
    "transformers process sequences in parallel efficiently.",
    "attention mechanisms focus on relevant tokens.",
    "self attention improves predictions across sequences.",
    "multihead attention allows diverse attention computations.",
    "positional encoding maintains sequential order.",
    "transformers outperform RNNs on many NLP tasks.",
    "language models learn patterns from large datasets.",
    "embedding layers map discrete tokens to continuous vectors.",
    "tokenization converts sentences into subword units.",
    "attention mechanisms capture important dependencies.",
    "transformers generalize better than traditional models.",
    "self attention allows models to consider all tokens simultaneously.",
    "multihead attention increases model capacity.",
    "positional encoding provides information about token positions.",
    "transformers efficiently process long sequences.",
    "language models predict next tokens accurately.",
    "embedding layers encode token semantics.",
    "tokenization divides text into meaningful units.",
    "attention mechanisms improve context modeling.",
    "self attention weighs tokens by relevance.",
    "multihead attention combines different attention computations.",
    "positional encoding informs model of sequence structure.",
    "transformers achieve state-of-the-art results in NLP.",
    "language models generate meaningful outputs.",
    "embedding layers convert tokens to vectors.",
    "tokenization enables model processing of text.",
    "attention mechanisms enhance sequence understanding.",
    "self attention improves token-level predictions.",
    "multihead attention captures multiple relationships.",
    "positional encoding preserves token order in sequences.",
    "transformers efficiently model sequences.",
    "language models generate coherent text.",
    "embedding layers provide dense token representations.",
    "tokenization prepares text for input into models.",
    "attention mechanisms focus on relevant parts of the sequence.",
    "self attention improves transformer predictions.",
    "multihead attention combines attention from several heads.",
    "positional encoding encodes sequential information for transformers.",
    "transformers capture dependencies between distant tokens.",
    "language models learn patterns and structures.",
    "embedding layers convert tokens into vector space.",
    "tokenization segments text into subword tokens.",
    "attention mechanisms help models understand context.",
    "self attention computes similarity between tokens.",
    "multihead attention improves learning of sequences.",
    "positional encoding adds sequence information to embeddings.",
    "transformers replace older recurrent architectures.",
    "language models predict text one token at a time.",
    "embedding layers encode semantic meaning of tokens.",
    "tokenization allows models to process text efficiently.",
    "attention mechanisms identify important tokens.",
    "self attention weights tokens by importance.",
    "multihead attention provides multiple perspectives.",
    "positional encoding maintains order of tokens.",
    "transformers excel in natural language processing.",
    "language models generate next tokens accurately.",
    "embedding layers represent tokens in vector space.",
    "tokenization converts sentences into model-ready tokens.",
    "attention mechanisms capture relationships in sequences.",
    "self attention considers all tokens simultaneously.",
    "multihead attention increases transformer capacity.",
    "positional encoding provides positional context.",
    "transformers model long-range dependencies effectively.",
    "language models learn patterns from sequences.",
    "embedding layers map discrete tokens to vectors.",
    "tokenization prepares inputs for neural networks.",
    "attention mechanisms improve context awareness.",
    "self attention enhances predictions.",
    "multihead attention allows diverse attention computations.",
    "positional encoding helps transformers maintain sequence order.",
]


texts += [
    # =================
    # Greetings (50 sentences)
    "hello world one more time.",
    "hi there one more time.",
    "good morning again everyone.",
    "good evening again everyone.",
    "hello world dear friends.",
    "hi world dear friends.",
    "greetings to all dear friends.",
    "welcome back dear friends.",
    "hello once again to all.",
    "hi once again to all.",
    "good night dear friends.",
    "hello buddy one more time.",
    "hi buddy one more time.",
    "greetings buddy one more time.",
    "good morning once more to friends.",
    "good evening once more to friends.",
    "hello world today once more.",
    "hi world today once more.",
    "hello friend today once more.",
    "greetings everyone today once more.",
    "good afternoon today again.",
    "hello everyone today again.",
    "hi everyone today again.",
    "greetings to all today again.",
    "welcome back dear buddy.",
    "hello again dear buddy.",
    "hi again dear buddy.",
    "good night again dear buddy.",
    "hello buddy today once more.",
    "hi buddy today once more.",
    "greetings buddy today once more.",
    "hello world my buddy today.",
    "hi world my buddy today.",
    "hello friend my buddy today.",
    "greetings everyone my buddy today.",
    "good morning my buddy today once more.",
    "good evening my buddy today once more.",
    "hello everyone my buddy today once more.",
    "hi everyone my buddy today once more.",
    "greetings to all my buddy today once more.",
    "welcome everyone my friend today once more.",
    "hello again everyone my friend today once more.",
    "hi again everyone my friend today once more.",
    "good night everyone my friend today once more.",
    "hello buddy my friend today once more.",
    "hi buddy my friend today once more.",
    "greetings buddy my friend today once more.",
    "hello world once again my buddy today.",
    "hi world once again my buddy today.",
    "hello friend once again my buddy today.",
    "greetings everyone once again my buddy today.",

    # =================
    # Machine Learning basics (150 sentences)
    "machine learning improves with practice.",
    "machine learning helps predict outcomes.",
    "deep learning models require many layers.",
    "deep learning captures hidden patterns.",
    "neural networks learn complex relationships.",
    "neural networks extract meaningful features.",
    "gradient descent minimizes errors.",
    "optimization algorithms improve accuracy.",
    "loss functions penalize incorrect predictions.",
    "accuracy evaluates prediction performance.",
    "models learn from examples iteratively.",
    "training adjusts weights efficiently.",
    "deep learning models learn hierarchically.",
    "neural networks approximate complex functions.",
    "machine learning predicts labels accurately.",
    "training requires many iterations.",
    "training optimizes network parameters.",
    "models generalize better with data.",
    "learning patterns improves predictions.",
    "neural networks encode input features.",
    # variations
    "machine learning models improve with data.",
    "deep learning helps neural networks learn patterns.",
    "gradient descent updates weights iteratively.",
    "accuracy measures correctness of predictions.",
    "models improve predictions after training.",
    "training neural networks optimizes parameters.",
    "optimization reduces prediction errors.",
    "loss functions guide neural network updates.",
    "neural networks extract useful representations.",
    "machine learning models generalize from examples.",
    "deep learning requires large datasets.",
    "neural networks approximate desired outputs.",
    "training improves prediction performance.",
    "evaluation checks neural network accuracy.",
    "validation ensures generalization on unseen data.",
    "models learn from diverse datasets.",
    "deep learning improves feature extraction.",
    "machine learning uses algorithms to predict outcomes.",
    "neural networks require activation functions.",
    "training adjusts model weights gradually.",
    "gradient descent iteratively updates parameters.",
    "models improve with repeated training cycles.",
    "training reduces loss effectively.",
    "learning rate affects model convergence speed.",
    "optimizers accelerate training process.",
    "data diversity enhances generalization.",
    "accuracy measures performance on test sets.",
    "loss functions evaluate errors.",
    "regularization prevents overfitting.",
    "dropout improves model robustness.",
    "batch normalization stabilizes training.",
    "large datasets improve training efficiency.",
    "models learn patterns from input data.",
    "examples teach models expected behavior.",
    "training requires repeated epochs.",
    "evaluation measures correctness.",
    "validation prevents memorization.",
    "supervised learning uses labeled data.",
    "unsupervised learning finds patterns in data.",
    "training improves network parameters iteratively.",
    "models learn from past examples.",
    "deep learning captures hierarchical features.",
    "neural networks map inputs to outputs accurately.",
    "optimization improves network performance.",
    "loss functions penalize prediction mistakes.",
    "accuracy evaluates network predictions.",
    "training uses mini-batches for efficiency.",
    "learning patterns help models generalize.",
    "gradient descent minimizes errors gradually.",
    "optimizers improve predictions consistently.",
    "regularization reduces overfitting.",
    "dropout randomly disables neurons.",
    "batch normalization normalizes layer outputs.",
    "learning rate schedules improve convergence.",
    "optimizers like Adam speed up learning.",
    "models learn representations effectively.",
    "training tunes neural network weights.",
    "evaluation checks model predictions.",
    "validation ensures generalization.",
    "training reduces prediction errors over time.",
    "gradient descent follows loss gradients.",
    "loss functions quantify prediction errors.",
    "accuracy tracks fraction of correct outputs.",
    "regularization encourages simpler models.",
    "dropout prevents co-adaptation of neurons.",
    "batch normalization stabilizes layer outputs.",
    "learning rate affects optimization speed.",
    "optimizers accelerate gradient updates.",
    "models learn from repeated examples.",
    "training improves performance metrics.",
    "evaluation provides feedback.",
    "validation confirms generalization.",
    "training iteratively updates parameters.",
    "gradient descent optimizes weights.",
    "loss functions measure errors.",
    "accuracy measures success.",
    "regularization prevents memorization.",
    "dropout improves robustness.",
    "batch normalization speeds up convergence.",
    "learning rate affects training.",
    "optimizers update weights.",
    "models learn patterns effectively.",
    "training adjusts weights gradually.",
    "evaluation measures accuracy on data.",
    "validation prevents overfitting to training set.",
    "training reduces errors iteratively.",
    "gradient descent improves parameters.",
    "loss functions guide updates.",
    "accuracy evaluates network predictions.",
    "regularization limits overfitting.",
    "dropout randomly disables units.",
    "batch normalization stabilizes learning.",
    "learning rate influences convergence speed.",
    "optimizers improve gradient updates.",
    "models learn features over time.",
    "training fine-tunes network weights.",
    "evaluation checks predictive performance.",
    "validation ensures model generalization.",
    "training reduces prediction loss effectively.",

    # =================
    # Transformers & Attention (100 sentences)
    "transformers are highly effective for NLP.",
    "attention focuses on relevant tokens.",
    "self attention improves context understanding.",
    "multihead attention captures multiple perspectives.",
    "positional encoding maintains sequence order.",
    "language models generate coherent text.",
    "language models predict next tokens.",
    "tokenization splits sentences into subwords.",
    "embedding layers convert tokens into vectors.",
    "transformers process sequences efficiently.",
    "attention captures long-range dependencies.",
    "language models learn from large corpora.",
    "self attention improves token predictions.",
    "transformers replace traditional RNNs.",
    "transformers are widely used in NLP.",
    "multihead attention allows parallel focus.",
    "self attention captures contextual relationships.",
    "positional encoding encodes token positions.",
    "transformers model sequences effectively.",
    "language models predict next token accurately.",
    # Variations
    "tokenization prepares text for modeling.",
    "embedding layers provide dense representations.",
    "transformers handle long sequences efficiently.",
    "attention mechanisms improve context awareness.",
    "transformers capture dependencies between tokens.",
    "language models generate coherent sequences.",
    "self attention improves transformer learning.",
    "transformers outperform RNNs in NLP tasks.",
    "positional encoding preserves order in sequences.",
    "multihead attention combines multiple attention heads.",
    "transformers are central to NLP research.",
    "language models learn patterns from text.",
    "embedding layers map tokens to vector space.",
    "transformers enable advanced NLP models.",
    "self attention allows context-sensitive predictions.",
    "attention mechanisms focus on important words.",
    "transformers learn dependencies between tokens.",
    "language models predict upcoming words.",
    "embedding layers transform input tokens.",
    "tokenization converts sentences to subwords.",
    "self attention calculates importance scores.",
    "multihead attention captures multiple relationships.",
    "positional encoding helps model sequence information.",
    "transformers replace older architectures like RNNs.",
    "language models create coherent text sequences.",
    "attention mechanisms guide transformer focus.",
    "transformers process text efficiently.",
    "embedding layers capture semantic meaning.",
    "language models complete sequences correctly.",
    "transformers learn complex token relationships.",
    "self attention captures contextual meaning.",
    "multihead attention attends to multiple tokens.",
    "positional encoding encodes sequential information.",
    "transformers are core components in NLP.",
    "language models improve with more data.",
    "embedding layers map words to vectors.",
    "tokenization segments text effectively.",
    "attention mechanisms improve understanding.",
    "transformers generalize well across sequences.",
    "language models predict words accurately.",
    "self attention calculates token importance.",
    "multihead attention combines multiple attentions.",
    "transformers are widely used in AI.",
    "positional encoding preserves sequence order.",
    "transformers learn patterns efficiently.",
    "language models understand context.",
    "embedding layers map inputs to vectors.",
    "attention helps models focus on relevant parts.",
    "transformers improve NLP performance.",
    "self attention allows context-aware predictions.",
    "multihead attention increases model capacity.",
    "language models generate meaningful sequences.",
    "embedding layers provide token representations.",
    "tokenization converts text to subwords.",
    "transformers outperform RNNs in sequences.",
    "attention captures long-range dependencies.",
]


texts += [
    # =================
    # Greetings (50 sentences)
    "hello everyone once again.",
    "hi world friends again.",
    "good morning friends once more.",
    "good evening everyone once more.",
    "hello world my dear friends again.",
    "hi world my dear friends again.",
    "greetings to all my dear friends again.",
    "welcome my dear friends once more.",
    "hello again to all my dear friends today.",
    "hi again to all my dear friends today.",
    "good night to everyone once more.",
    "hello buddy again today.",
    "hi buddy again today.",
    "greetings buddy again today.",
    "good morning again my friends today.",
    "good evening again my friends today.",
    "hello world today once again.",
    "hi world today once again.",
    "hello friend today once again.",
    "greetings everyone today once again.",
    "good afternoon friends once again.",
    "hello everyone again today.",
    "hi everyone again today.",
    "greetings to all again today.",
    "welcome back my dear friends again.",
    "hello again my dear friends today.",
    "hi again my dear friends today.",
    "good night again my dear friends today.",
    "hello buddy once more today.",
    "hi buddy once more today.",
    "greetings buddy once more today.",
    "hello world my buddy again today.",
    "hi world my buddy again today.",
    "hello friend my buddy again today.",
    "greetings everyone my buddy again today.",
    "good morning my buddy again today.",
    "good evening my buddy again today.",
    "hello everyone my buddy again today.",
    "hi everyone my buddy again today.",
    "greetings to all my buddy again today.",
    "welcome everyone my friend again today.",
    "hello again everyone my friend again today.",
    "hi again everyone my friend again today.",
    "good night everyone my friend again today.",
    "hello buddy my friend again today.",
    "hi buddy my friend again today.",
    "greetings buddy my friend again today.",
    "hello world once again my buddy today.",
    "hi world once again my buddy today.",
    "hello friend once again my buddy today.",
    "greetings everyone once again my buddy today.",

    # =================
    # Machine Learning basics (150 sentences)
    "machine learning predicts outcomes accurately today.",
    "deep learning captures patterns effectively.",
    "neural networks learn features efficiently over time.",
    "gradient descent optimizes network weights iteratively.",
    "optimization improves model performance consistently.",
    "loss functions measure errors precisely.",
    "accuracy tracks correct predictions effectively.",
    "models learn from labeled examples successfully.",
    "training improves neural network performance gradually.",
    "deep learning models capture complex hidden patterns.",
    "neural networks predict outputs accurately and consistently.",
    "machine learning predicts labels correctly over many examples.",
    "training requires multiple examples to converge successfully.",
    "training adjusts weights for better model learning.",
    "models generalize from more data examples successfully.",
    "learning patterns improves predictions accurately.",
    "neural networks extract useful features from input data.",
    "machine learning models make accurate predictions consistently.",
    "deep learning models require GPUs for faster training.",
    "neural networks approximate complex functions efficiently.",
    # variations
    "machine learning is fun and very effective.",
    "deep learning helps neural networks learn more efficiently.",
    "gradient descent finds global minima efficiently.",
    "accuracy evaluates neural network performance accurately.",
    "models improve predictions with larger datasets.",
    "training neural networks requires patience and time.",
    "optimization algorithms adjust weights effectively.",
    "loss functions guide neural network learning process.",
    "neural networks learn hidden features efficiently.",
    "machine learning models make accurate predictions reliably.",
    "deep learning models need multiple layers for accuracy.",
    "neural networks approximate complex functions successfully.",
    "training improves prediction accuracy and generalization.",
    "evaluation checks model performance accurately.",
    "validation prevents overfitting effectively.",
    "models learn from diverse examples successfully.",
    "deep learning improves feature extraction efficiently.",
    "machine learning uses various algorithms consistently.",
    "neural networks require activations and layers.",
    "training optimizes parameters over multiple iterations.",
    "gradient descent updates weights efficiently over time.",
    "models improve with repeated training cycles effectively.",
    "training minimizes loss consistently over epochs.",
    "learning rate affects convergence speed significantly.",
    "optimizers help models converge faster effectively.",
    "data improves generalization capabilities of models.",
    "accuracy measures correctness of predictions accurately.",
    "loss functions guide optimization during training.",
    "regularization prevents overfitting effectively.",
    "dropout improves neural network generalization significantly.",
    "batch normalization stabilizes and accelerates training.",
    "large datasets improve model training effectively.",
    "models learn patterns from multiple input examples.",
    "examples teach models expected behaviors accurately.",
    "training requires repeated epochs for convergence.",
    "evaluation measures correctness and performance accurately.",
    "validation ensures models generalize properly.",
    "supervised learning uses labeled datasets successfully.",
    "unsupervised learning discovers hidden patterns in data.",
    "training neural networks is iterative and incremental.",
    "models learn from previous examples effectively.",
    "deep learning models capture complex features from data efficiently.",
    "neural networks map inputs to outputs accurately consistently.",
    "optimization algorithms improve prediction quality over time.",
    "loss functions penalize incorrect predictions effectively.",
    "accuracy measures model success reliably.",
    "training uses mini-batches for efficiency consistently.",
    "learning patterns helps models generalize effectively.",
    "gradient descent minimizes loss efficiently over iterations.",
    "optimization algorithms improve predictions consistently over time.",
    "regularization techniques reduce overfitting significantly.",
    "dropout randomly disables neurons during training to generalize effectively.",
    "batch normalization improves training stability and speed.",
    "training improves network weights and biases iteratively.",
    "models learn weights from examples accurately.",
    "deep learning models require multiple hidden layers.",
    "machine learning predicts outcomes with high accuracy.",
    "training improves performance metrics reliably.",
    "evaluation verifies model predictions effectively.",
    "validation prevents overfitting and memorization successfully.",
    "models generalize from multiple examples efficiently.",
    "training adjusts weights and biases correctly over epochs.",
    "learning rate impacts model convergence consistently.",
    "optimizers help networks learn faster and efficiently.",
    "data diversity improves generalization of models.",
    "examples teach neural networks patterns effectively.",
    "loss functions guide accurate optimization consistently.",
    "accuracy indicates correct predictions reliably.",
    "supervised learning maps inputs to outputs effectively.",
    "unsupervised learning discovers latent structures accurately.",
    "training improves model parameters iteratively and reliably.",
    "models adapt from examples efficiently and accurately.",
    "deep learning captures complex patterns effectively.",
    "machine learning predicts meaningful patterns consistently.",
    "neural networks approximate desired functions accurately.",
    "gradient descent updates weights iteratively and effectively.",
    "training minimizes errors gradually over iterations.",
    "evaluation assesses model predictions reliably.",
    "validation confirms generalization ability accurately.",
    "models learn features from multiple examples effectively.",
    "examples guide optimization process consistently.",
    "training enhances neural network performance significantly.",
    "learning improves model predictions consistently.",
    "neural networks optimize predictions accurately and efficiently.",
    "deep learning models enhance prediction accuracy.",
    "machine learning improves predictive insights effectively.",
    "training teaches patterns to models efficiently consistently.",
    "evaluation measures predictive results accurately.",
    "validation confirms model correctness reliably.",
    "models learn from many examples over time effectively.",
    "training neural networks improves weight initialization successfully.",
    "deep learning requires large datasets for accurate learning.",
    "machine learning predicts labels for unseen examples reliably.",
    "neural networks map inputs to outputs effectively.",
    "gradient descent updates weights step by step efficiently.",
    "training minimizes errors across multiple epochs consistently.",
    "evaluation assesses performance on unseen data reliably.",
    "validation ensures generalization to new examples effectively.",
    "models learn features across many input examples consistently.",
    "examples guide learning in neural networks effectively.",
    "training enhances overall model performance gradually.",
    "learning improves neural network predictions reliably.",
    "neural networks optimize features effectively consistently.",
    "deep learning models improve generalization with data effectively.",
    "machine learning provides predictive insights consistently reliably.",
    "training teaches networks patterns over repeated epochs successfully.",
    "evaluation measures model accuracy effectively consistently.",
    "validation confirms network performance reliably consistently.",
]

